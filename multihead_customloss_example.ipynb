{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Generate dummy data with separate lists for X and y\n",
    "X_full = tf.random.normal(shape=(100, 32, 32, 3))\n",
    "y_full = [tf.random.uniform(shape=(100,)) for _ in range(3)]  # List of target variables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100,)\n",
      "(100,)\n",
      "(100, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in y_full:\n",
    "    print(i.shape)\n",
    "print(X_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,) (20,)\n",
      "(80,) (20,)\n",
      "(80,) (20,)\n",
      "(80, 32, 32, 3) (20, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy\n",
    "X_full_np = X_full.numpy()\n",
    "\n",
    "# Stack the arrays vertically to create a single array of shape (3, 100)\n",
    "y_full_stacked = np.vstack(y_full).T  # Transpose to get shape (100, 3)\n",
    "\n",
    "# Convert y_full_stacked to integers\n",
    "y_full_stacked = y_full_stacked.astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full_np, y_full_stacked, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Split y_train and y_test back into three separate arrays\n",
    "y1_train, y2_train, y3_train = y_train[:, 0], y_train[:, 1], y_train[:, 2]\n",
    "y1_test, y2_test, y3_test = y_test[:, 0], y_test[:, 1], y_test[:, 2]\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(y1_train.shape, y1_test.shape)\n",
    "print(y2_train.shape, y2_test.shape)\n",
    "print(y3_train.shape, y3_test.shape)\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "  inputs = keras.Input(shape=(32, 32, 3))\n",
    "  x = layers.Conv2D(32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "  x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  outputs = []\n",
    "  # Define separate heads for each target variable\n",
    "  for _ in range(3):\n",
    "    branch = layers.Dense(64, activation=\"relu\")(x)\n",
    "    branch = layers.Dense(1)(branch)\n",
    "    outputs.append(branch)\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 30, 30, 32)   896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 15, 15, 32)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 7200)         0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           460864      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           460864      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           460864      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            65          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            65          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,383,683\n",
      "Trainable params: 1,383,683\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 4s 300ms/step - loss: 199.7172 - dense_1_loss: 39.9720 - dense_3_loss: 72.5805 - dense_5_loss: 87.1646 - dense_1_mae: 4.9757 - dense_3_mae: 6.3674 - dense_5_mae: 6.7568 - val_loss: 27.5915 - val_dense_1_loss: 6.7962 - val_dense_3_loss: 11.2565 - val_dense_5_loss: 9.5388 - val_dense_1_mae: 2.5967 - val_dense_3_mae: 3.3379 - val_dense_5_mae: 3.0680\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 15.9477 - dense_1_loss: 3.6654 - dense_3_loss: 5.8258 - dense_5_loss: 6.4565 - dense_1_mae: 1.6748 - dense_3_mae: 1.7629 - dense_5_mae: 2.3739 - val_loss: 5.0917 - val_dense_1_loss: 1.2341 - val_dense_3_loss: 0.0768 - val_dense_5_loss: 3.7808 - val_dense_1_mae: 1.1047 - val_dense_3_mae: 0.2766 - val_dense_5_mae: 1.9429\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.7541 - dense_1_loss: 1.3471 - dense_3_loss: 0.0947 - dense_5_loss: 4.3123 - dense_1_mae: 1.1561 - dense_3_mae: 0.3049 - dense_5_mae: 2.0714 - val_loss: 6.8957 - val_dense_1_loss: 1.1647 - val_dense_3_loss: 0.1524 - val_dense_5_loss: 5.5785 - val_dense_1_mae: 1.0730 - val_dense_3_mae: 0.3901 - val_dense_5_mae: 2.3605\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.8896 - dense_1_loss: 1.0090 - dense_3_loss: 0.1606 - dense_5_loss: 5.7201 - dense_1_mae: 0.9956 - dense_3_mae: 0.3999 - dense_5_mae: 2.3896 - val_loss: 6.4190 - val_dense_1_loss: 0.4073 - val_dense_3_loss: 0.1882 - val_dense_5_loss: 5.8235 - val_dense_1_mae: 0.6282 - val_dense_3_mae: 0.4335 - val_dense_5_mae: 2.4119\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.1152 - dense_1_loss: 0.2901 - dense_3_loss: 0.1850 - dense_5_loss: 5.6401 - dense_1_mae: 0.5063 - dense_3_mae: 0.4296 - dense_5_mae: 2.3729 - val_loss: 5.1250 - val_dense_1_loss: 0.0161 - val_dense_3_loss: 0.1819 - val_dense_5_loss: 4.9270 - val_dense_1_mae: 0.0876 - val_dense_3_mae: 0.4262 - val_dense_5_mae: 2.2184\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.7540 - dense_1_loss: 0.0072 - dense_3_loss: 0.1712 - dense_5_loss: 4.5756 - dense_1_mae: 0.0493 - dense_3_mae: 0.4131 - dense_5_mae: 2.1362 - val_loss: 3.6704 - val_dense_1_loss: 2.8869e-06 - val_dense_3_loss: 0.1492 - val_dense_5_loss: 3.5212 - val_dense_1_mae: 0.0017 - val_dense_3_mae: 0.3859 - val_dense_5_mae: 1.8751\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 3.2810 - dense_1_loss: 2.8992e-06 - dense_3_loss: 0.1356 - dense_5_loss: 3.1454 - dense_1_mae: 0.0017 - dense_3_mae: 0.3675 - dense_5_mae: 1.7688 - val_loss: 2.2219 - val_dense_1_loss: 2.9337e-06 - val_dense_3_loss: 0.1071 - val_dense_5_loss: 2.1148 - val_dense_1_mae: 0.0017 - val_dense_3_mae: 0.3269 - val_dense_5_mae: 1.4527\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.9021 - dense_1_loss: 2.9436e-06 - dense_3_loss: 0.0945 - dense_5_loss: 1.8076 - dense_1_mae: 0.0017 - dense_3_mae: 0.3060 - dense_5_mae: 1.3363 - val_loss: 1.0812 - val_dense_1_loss: 2.9711e-06 - val_dense_3_loss: 0.0678 - val_dense_5_loss: 1.0134 - val_dense_1_mae: 0.0017 - val_dense_3_mae: 0.2600 - val_dense_5_mae: 1.0047\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.8653 - dense_1_loss: 2.9790e-06 - dense_3_loss: 0.0576 - dense_5_loss: 0.8077 - dense_1_mae: 0.0017 - dense_3_mae: 0.2387 - dense_5_mae: 0.8896 - val_loss: 0.3677 - val_dense_1_loss: 3.0010e-06 - val_dense_3_loss: 0.0375 - val_dense_5_loss: 0.3302 - val_dense_1_mae: 0.0017 - val_dense_3_mae: 0.1930 - val_dense_5_mae: 0.5715\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.2646 - dense_1_loss: 3.0072e-06 - dense_3_loss: 0.0304 - dense_5_loss: 0.2342 - dense_1_mae: 0.0017 - dense_3_mae: 0.1727 - dense_5_mae: 0.4648 - val_loss: 0.0544 - val_dense_1_loss: 3.0245e-06 - val_dense_3_loss: 0.0172 - val_dense_5_loss: 0.0372 - val_dense_1_mae: 0.0017 - val_dense_3_mae: 0.1306 - val_dense_5_mae: 0.1855\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0545 - dense_1_loss: 3.0245e-06 - dense_3_loss: 0.0154 - dense_5_loss: 0.0391 - dense_1_mae: 0.0017 - dense_3_mae: 0.1231 - dense_5_mae: 0.1896\n",
      "Loss: 0.05450376123189926\n",
      "Metric 1: 3.0245084872149164e-06\n",
      "Metric 2: 0.015376227907836437\n",
      "Metric 3: 0.0391245111823082\n",
      "Metric 4: 0.0017391113797202706\n",
      "Metric 5: 0.12308566272258759\n",
      "Metric 6: 0.18960729241371155\n"
     ]
    }
   ],
   "source": [
    "# scenario 1 full dataset and loss function accessing to full y1, y2 and y3 rows\n",
    "#\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model on the full dataset\n",
    "model.fit(X_train, [y1_train, y2_train, y3_train], epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "#model.evaluate(X_test, [y1_test, y2_test, y3_test])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss_values, *metrics_values = model.evaluate(X_test, [y1_test, y2_test, y3_test])\n",
    "\n",
    "# Print the loss value\n",
    "print(\"Loss:\", loss_values)\n",
    "\n",
    "# Print the metric values (e.g., mean absolute error)\n",
    "for i, metric_value in enumerate(metrics_values):\n",
    "    print(f\"Metric {i+1}:\", metric_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 278ms/step - loss: 269.4718 - dense_0_loss: 129.6439 - dense_1_loss: 68.3776 - dense_2_loss: 71.4503 - dense_0_mae: 8.2016 - dense_1_mae: 6.0234 - dense_2_mae: 6.2637 - val_loss: 34.7629 - val_dense_0_loss: 11.8030 - val_dense_1_loss: 11.7188 - val_dense_2_loss: 11.2411 - val_dense_0_mae: 3.4160 - val_dense_1_mae: 3.4025 - val_dense_2_mae: 3.3399\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 18.6530 - dense_0_loss: 7.2478 - dense_1_loss: 5.7005 - dense_2_loss: 5.7047 - dense_0_mae: 2.3657 - dense_1_mae: 1.9183 - dense_2_mae: 2.0029 - val_loss: 4.0098 - val_dense_0_loss: 2.1331 - val_dense_1_loss: 0.7243 - val_dense_2_loss: 1.1524 - val_dense_0_mae: 1.4541 - val_dense_1_mae: 0.8471 - val_dense_2_mae: 1.0798\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.3108 - dense_0_loss: 2.2761 - dense_1_loss: 0.7773 - dense_2_loss: 1.2574 - dense_0_mae: 1.5040 - dense_1_mae: 0.8734 - dense_2_mae: 1.1145 - val_loss: 4.5661 - val_dense_0_loss: 2.1536 - val_dense_1_loss: 1.0137 - val_dense_2_loss: 1.3988 - val_dense_0_mae: 1.4613 - val_dense_1_mae: 1.0037 - val_dense_2_mae: 1.1867\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.0031 - dense_0_loss: 1.8607 - dense_1_loss: 0.8985 - dense_2_loss: 1.2439 - dense_0_mae: 1.3531 - dense_1_mae: 0.9452 - dense_2_mae: 1.1108 - val_loss: 2.5290 - val_dense_0_loss: 0.9123 - val_dense_1_loss: 0.7858 - val_dense_2_loss: 0.8309 - val_dense_0_mae: 0.9461 - val_dense_1_mae: 0.8830 - val_dense_2_mae: 0.9126\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 1.8945 - dense_0_loss: 0.6449 - dense_1_loss: 0.6168 - dense_2_loss: 0.6328 - dense_0_mae: 0.7791 - dense_1_mae: 0.7806 - dense_2_mae: 0.7841 - val_loss: 0.7084 - val_dense_0_loss: 0.0858 - val_dense_1_loss: 0.3857 - val_dense_2_loss: 0.2369 - val_dense_0_mae: 0.2634 - val_dense_1_mae: 0.6161 - val_dense_2_mae: 0.4727\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4351 - dense_0_loss: 0.0438 - dense_1_loss: 0.2562 - dense_2_loss: 0.1351 - dense_0_mae: 0.1490 - dense_1_mae: 0.4968 - dense_2_mae: 0.3331 - val_loss: 0.0982 - val_dense_0_loss: 5.2524e-06 - val_dense_1_loss: 0.0948 - val_dense_2_loss: 0.0034 - val_dense_0_mae: 0.0023 - val_dense_1_mae: 0.2978 - val_dense_2_mae: 0.0361\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.0445 - dense_0_loss: 2.3284e-05 - dense_1_loss: 0.0418 - dense_2_loss: 0.0027 - dense_0_mae: 0.0028 - dense_1_mae: 0.1809 - dense_2_mae: 0.0189 - val_loss: 0.0034 - val_dense_0_loss: 5.6694e-06 - val_dense_1_loss: 0.0034 - val_dense_2_loss: 5.0679e-06 - val_dense_0_mae: 0.0024 - val_dense_1_mae: 0.0339 - val_dense_2_mae: 0.0023\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 9.3185e-04 - dense_0_loss: 5.7611e-06 - dense_1_loss: 9.2098e-04 - dense_2_loss: 5.1021e-06 - dense_0_mae: 0.0024 - dense_1_mae: 0.0071 - dense_2_mae: 0.0023 - val_loss: 1.7969e-05 - val_dense_0_loss: 6.0214e-06 - val_dense_1_loss: 6.7495e-06 - val_dense_2_loss: 5.1982e-06 - val_dense_0_mae: 0.0025 - val_dense_1_mae: 0.0026 - val_dense_2_mae: 0.0023\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1.8115e-05 - dense_0_loss: 6.1125e-06 - dense_1_loss: 6.7764e-06 - dense_2_loss: 5.2263e-06 - dense_0_mae: 0.0025 - dense_1_mae: 0.0026 - dense_2_mae: 0.0023 - val_loss: 1.8475e-05 - val_dense_0_loss: 6.3177e-06 - val_dense_1_loss: 6.8518e-06 - val_dense_2_loss: 5.3052e-06 - val_dense_0_mae: 0.0025 - val_dense_1_mae: 0.0026 - val_dense_2_mae: 0.0023\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.8584e-05 - dense_0_loss: 6.3826e-06 - dense_1_loss: 6.8736e-06 - dense_2_loss: 5.3282e-06 - dense_0_mae: 0.0025 - dense_1_mae: 0.0026 - dense_2_mae: 0.0023 - val_loss: 1.8894e-05 - val_dense_0_loss: 6.5663e-06 - val_dense_1_loss: 6.9346e-06 - val_dense_2_loss: 5.3927e-06 - val_dense_0_mae: 0.0026 - val_dense_1_mae: 0.0026 - val_dense_2_mae: 0.0023\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 1.8751e-05 - dense_0_loss: 6.5663e-06 - dense_1_loss: 6.7923e-06 - dense_2_loss: 5.3927e-06 - dense_0_mae: 0.0026 - dense_1_mae: 0.0026 - dense_2_mae: 0.0023\n",
      "Loss: 1.875126145023387e-05\n",
      "Metric 1: 6.566290721821133e-06\n",
      "Metric 2: 6.792250587750459e-06\n",
      "Metric 3: 5.392719685914926e-06\n",
      "Metric 4: 0.0025624774862080812\n",
      "Metric 5: 0.00260278326459229\n",
      "Metric 6: 0.002322223037481308\n"
     ]
    }
   ],
   "source": [
    "# Scenario 2: our target situation: dummy case reducing access to y3 values by with custom loss function in 'head3'\n",
    "\n",
    "def create_model1():\n",
    "    inputs = keras.Input(shape=(32, 32, 3))\n",
    "    x = layers.Conv2D(32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = []\n",
    "    # Define separate heads for each target variable\n",
    "    for i in range(3):\n",
    "        branch = layers.Dense(64, activation=\"relu\", name=f'dense_{i}_out')(x)\n",
    "        branch = layers.Dense(1, name=f'dense_{i}')(branch)\n",
    "        outputs.append(branch)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model1 = create_model1()\n",
    "\n",
    "# Define custom loss functions for each output\n",
    "def custom_loss_0(y_true, y_pred):\n",
    "    return keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def custom_loss_1(y_true, y_pred):\n",
    "    return keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def custom_loss_2(y_true, y_pred):\n",
    "    threshold = 0.8 ##example here gor 80%, but this should be iterated as 20, 40, 60, 80% \n",
    "    mask = tf.random.uniform(shape=tf.shape(y_true)) < threshold\n",
    "    masked_true = tf.boolean_mask(y_true, mask)\n",
    "    masked_pred = tf.boolean_mask(y_pred, mask)\n",
    "    return keras.losses.mean_squared_error(masked_true, masked_pred)\n",
    "\n",
    "custom_losses = {\n",
    "    'dense_0': custom_loss_0,\n",
    "    'dense_1': custom_loss_1,\n",
    "    'dense_2': custom_loss_2\n",
    "}\n",
    "\n",
    "# Compile the model with custom losses for each output\n",
    "model1.compile(optimizer='adam', loss=custom_losses, metrics=['mae'])\n",
    "\n",
    "# Train the model with masked y3_train\n",
    "model1.fit(X_train, {'dense_0': y1_train, 'dense_1': y2_train, 'dense_2': y3_train}, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss_values, *metrics_values = model1.evaluate(X_test, [y1_test, y2_test, y3_test])\n",
    "\n",
    "# Print the loss value\n",
    "print(\"Loss:\", loss_values)\n",
    "\n",
    "# Print the metric values (e.g., mean absolute error)\n",
    "for i, metric_value in enumerate(metrics_values):\n",
    "    print(f\"Metric {i+1}:\", metric_value)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
